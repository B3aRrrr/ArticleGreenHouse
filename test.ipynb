{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "from GreenHouseEnv.envTHumidpHEC import GreenHousePCSEEnvVer2\n",
    "from pcse_gym.envs.common_env import  PCSEEnv\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.summary import create_file_writer, scalar\n",
    "from stable_baselines3 import A2C, SAC, PPO, TD3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from datetime import datetime \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard import program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0, writer=None):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.rewards = []\n",
    "        self.writer = writer  # Передайте SummaryWriter\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.model.ep_info_buffer:\n",
    "            reward = self.model.ep_info_buffer[0].get('r', 0)  # Получаем значение 'r' из первого элемента или 0, если 'r' отсутствует\n",
    "        else:\n",
    "            reward = 0  # Обработка случая, когда очередь пуста\n",
    "\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "        # Запишите вознаграждение в SummaryWriter\n",
    "        with self.writer.as_default():\n",
    "            scalar(\"Reward\", reward, step=self.num_timesteps)\n",
    "\n",
    "        # Здесь вы также можете записывать другие параметры среды\n",
    "\n",
    "        return True\n",
    "\n",
    "    def close(self) -> None:\n",
    "        pass \n",
    "env = GreenHousePCSEEnvVer2(  timestep=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO (Proximal Policy Optimization):\n",
    "```python\n",
    "total_timesteps (int): #Общее количество шагов времени, в течение которых будет произведено обучение.\n",
    "callback (BaseCallback, optional):# Обратный вызов (callback) для записи промежуточных результатов обучения.\n",
    "log_interval (int):# Интервал для записи логов.\n",
    "eval_env (gym.Env, optional): #Среда для оценки производительности модели.\n",
    "eval_freq (int): #Частота оценки (в терминах количества шагов).\n",
    "n_eval_episodes (int): #Количество эпизодов для оценки модели.\n",
    "#Множество других аргументов, связанных с настройками обучения и гиперпараметрами.\n",
    "```\n",
    "\n",
    "\n",
    "SAC (Soft Actor-Critic):\n",
    "```python\n",
    "total_timesteps (int): # Общее количество шагов времени, в течение которых будет произведено обучение.\n",
    "callback (BaseCallback, optional):#  Обратный вызов (callback) для записи промежуточных результатов обучения.\n",
    "log_interval (int): #Интервал для записи логов.\n",
    "eval_env (gym.Env, optional): #Среда для оценки производительности модели.\n",
    "eval_freq (int): #Частота оценки (в терминах количества шагов).\n",
    "n_eval_episodes (int): #Количество эпизодов для оценки модели.\n",
    "# Множество других аргументов, связанных с настройками обучения и гиперпараметрами.\n",
    "```\n",
    "\n",
    "TD3 (Twin Delayed Deep Deterministic Policy Gradient):\n",
    "```python\n",
    "total_timesteps (int): # Общее количество шагов времени, в течение которых будет произведено обучение.\n",
    "callback (BaseCallback, optional): #Обратный вызов (callback) для записи промежуточных результатов обучения.\n",
    "log_interval (int):# Интервал для записи логов.\n",
    "eval_env (gym.Env, optional):# Среда для оценки производительности модели.\n",
    "eval_freq (int): #Частота оценки (в терминах количества шагов).\n",
    "n_eval_episodes (int): #Количество эпизодов для оценки модели.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_LOGS_DIR=os.path.join(\n",
    "    os.getcwd(),\"tensorboard_logs\"\n",
    "    )\n",
    "if not os.path.exists(TENSORBOARD_LOGS_DIR):\n",
    "    os.makedirs(TENSORBOARD_LOGS_DIR)\n",
    "ENV_DIR = os.path.join(TENSORBOARD_LOGS_DIR,env.__class__.name)\n",
    "if not os.path.exists(ENV_DIR):\n",
    "    os.makedirs(ENV_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard --logdir={MODEL_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 5e4\n",
    "models_variants = {\n",
    "    'sac': {\n",
    "    'batch_size': 64,\n",
    "    'ent_coef': 'auto',\n",
    "    'learning_rate': 0.0003,  # Снижаем learning rate\n",
    "    'learning_starts': 1000,  # Увеличиваем learning_starts\n",
    "    'buffer_size': int(1/4*total_timesteps),\n",
    "},\n",
    "    'a2c': {\n",
    "    'n_steps': 5,\n",
    "    'ent_coef': 0.01,\n",
    "    'learning_rate': 0.0007,\n",
    "    'vf_coef': 0.25,\n",
    "    'max_grad_norm': 0.5,\n",
    "},\n",
    "    'td3': {\n",
    "    \"learning_rate\": 1e-4,  # Увеличиваем learning rate\n",
    "    \"buffer_size\": 1000000,  # Увеличиваем buffer_size\n",
    "    \"learning_starts\": 1000,  # Увеличиваем learning_starts\n",
    "    \"batch_size\": 128,  # Увеличиваем batch_size\n",
    "    \"tau\": 0.005,  # Уменьшаем tau\n",
    "    \"gamma\": 0.99,  # Увеличиваем gamma\n",
    "    \"train_freq\": (1, \"episode\"),  # Увеличиваем train_freq\n",
    "    \"gradient_steps\": 100,  # Увеличиваем gradient_steps\n",
    "    \"policy_delay\": 2,  # Уменьшаем policy_delay\n",
    "    \"target_policy_noise\": 0.2,  # Увеличиваем target_policy_noise\n",
    "    \"target_noise_clip\": 0.5,  # Увеличиваем target_noise_clip\n",
    "    \"stats_window_size\": 100,\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\test.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/ArticleGreenHouse/test.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Создайте SummaryWriter\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/ArticleGreenHouse/test.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m writer \u001b[39m=\u001b[39m create_file_writer(log_dir)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/ArticleGreenHouse/test.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/ArticleGreenHouse/test.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps, \u001b[39m# Общее количество шагов времени, в течение которых будет произведено обучение.\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/ArticleGreenHouse/test.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     callback \u001b[39m=\u001b[39;49m CustomCallback(writer\u001b[39m=\u001b[39;49mwriter),\u001b[39m#  Обратный вызов (callback) для записи промежуточных результатов обучения.\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/ArticleGreenHouse/test.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     log_interval\u001b[39m=\u001b[39;49m\u001b[39m1e1\u001b[39;49m, \u001b[39m#Интервал для записи логов.\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/ArticleGreenHouse/test.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/ArticleGreenHouse/test.ipynb#X13sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/ArticleGreenHouse/test.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m model\u001b[39m.\u001b[39msave(path\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(MODEL_DIR_VAR,\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mvar\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/ArticleGreenHouse/test.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m logdirs\u001b[39m.\u001b[39mappend(log_dir)\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\venv\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    299\u001b[0m     \u001b[39mself\u001b[39m: SelfSAC,\n\u001b[0;32m    300\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    306\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfSAC:\n\u001b[1;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    308\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    309\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    310\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    311\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    312\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    313\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    314\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\venv\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:312\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    309\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    311\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 312\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[0;32m    313\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[0;32m    314\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[0;32m    315\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[0;32m    316\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    317\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[0;32m    318\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[0;32m    319\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    320\u001b[0m     )\n\u001b[0;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\venv\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:554\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    551\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[0;32m    552\u001b[0m \u001b[39m# print(f'[OffPolicyAlgorithm [l_548]] actions = {actions}')\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 554\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m    555\u001b[0m \u001b[39m# print(f'[OffPolicyAlgorithm [l_552]] new_obs = {new_obs}')\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \n\u001b[0;32m    557\u001b[0m \u001b[39m# for i in infos:\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[39m#     print(f'[OffPolicyAlgorithm [l_552]] i = {i[\"terminal_observation\"]}')\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\venv\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\venv\\lib\\site-packages\\shimmy\\openai_gym_compatibility.py:123\u001b[0m, in \u001b[0;36mGymV26CompatibilityV0.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[ObsType, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgym_env\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\GreenHouseEnv\\envTHumidpHEC.py:229\u001b[0m, in \u001b[0;36mGreenHousePCSEEnvVer2.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    223\u001b[0m info \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m() \n\u001b[0;32m    225\u001b[0m current_args \u001b[39m=\u001b[39m {\n\u001b[0;32m    226\u001b[0m     var:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_observation()[var][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m var \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m_additional_variables\n\u001b[0;32m    227\u001b[0m } \n\u001b[1;32m--> 229\u001b[0m current_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_action(action,current_args) \n\u001b[0;32m    230\u001b[0m \u001b[39m# Get the model output\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_saved_days(current_args) \n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\GreenHouseEnv\\envTHumidpHEC.py:281\u001b[0m, in \u001b[0;36mGreenHousePCSEEnvVer2._apply_action\u001b[1;34m(self, action, current_args)\u001b[0m\n\u001b[0;32m    278\u001b[0m current_args[\u001b[39m'\u001b[39m\u001b[39mpH\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(current_args[\u001b[39m'\u001b[39m\u001b[39mpH\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m (_pH  \u001b[39m-\u001b[39m current_args[\u001b[39m'\u001b[39m\u001b[39mpH\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV_soil),\u001b[39m0\u001b[39m,\u001b[39m14\u001b[39m)\n\u001b[0;32m    279\u001b[0m \u001b[39m#endregion\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[39m#region 4. Apply Temp and humid\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m current_args[\u001b[39m'\u001b[39m\u001b[39mT_soil\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m temp_soil(\n\u001b[0;32m    282\u001b[0m     dt\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m24\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timestep, \n\u001b[0;32m    283\u001b[0m     T_soil_0\u001b[39m=\u001b[39;49mcurrent_args[\u001b[39m'\u001b[39;49m\u001b[39mT_soil\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m    284\u001b[0m     T_air\u001b[39m=\u001b[39;49mT_air\n\u001b[0;32m    285\u001b[0m     )\n\u001b[0;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiff_humid_time\u001b[39m(theta_t0,t, theta_air, V_soil, N, E):\n\u001b[0;32m    287\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m    Параметры:\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m    t - текущее время (в сутках)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39m    E - естественное испарение жидкости из почвы (в литрах в сутки) \u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m    '''\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\GreenHouseEnv\\calcsMethods.py:57\u001b[0m, in \u001b[0;36mtemp_soil\u001b[1;34m(dt, T_soil_0, R, h, T_air, _lambda)\u001b[0m\n\u001b[0;32m     54\u001b[0m t_span \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, dt)\n\u001b[0;32m     55\u001b[0m T_initial \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((GRID_SIZE, GRID_SIZE)) \u001b[39m*\u001b[39m T_soil_0\n\u001b[1;32m---> 57\u001b[0m sol \u001b[39m=\u001b[39m solve_ivp(\n\u001b[0;32m     58\u001b[0m     \u001b[39mlambda\u001b[39;49;00m t, T: heat_equation(t, T, r, z, alpha_r, alpha_z, T_air),\n\u001b[0;32m     59\u001b[0m     t_span,\n\u001b[0;32m     60\u001b[0m     T_initial\u001b[39m.\u001b[39;49mflatten(),\n\u001b[0;32m     61\u001b[0m     t_eval\u001b[39m=\u001b[39;49m[dt]\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mclip(sol\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mmean(), \u001b[39m0\u001b[39m, \u001b[39m40\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\venv\\lib\\site-packages\\scipy\\integrate\\_ivp\\ivp.py:602\u001b[0m, in \u001b[0;36msolve_ivp\u001b[1;34m(fun, t_span, y0, method, t_eval, dense_output, events, vectorized, args, **options)\u001b[0m\n\u001b[0;32m    600\u001b[0m status \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[39mwhile\u001b[39;00m status \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 602\u001b[0m     message \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    604\u001b[0m     \u001b[39mif\u001b[39;00m solver\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfinished\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    605\u001b[0m         status \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\venv\\lib\\site-packages\\scipy\\integrate\\_ivp\\base.py:197\u001b[0m, in \u001b[0;36mOdeSolver.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt\n\u001b[1;32m--> 197\u001b[0m     success, message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_impl()\n\u001b[0;32m    199\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m success:\n\u001b[0;32m    200\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfailed\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\venv\\lib\\site-packages\\scipy\\integrate\\_ivp\\rk.py:144\u001b[0m, in \u001b[0;36mRungeKutta._step_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m h \u001b[39m=\u001b[39m t_new \u001b[39m-\u001b[39m t\n\u001b[0;32m    142\u001b[0m h_abs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(h)\n\u001b[1;32m--> 144\u001b[0m y_new, f_new \u001b[39m=\u001b[39m rk_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun, t, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf, h, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mA,\n\u001b[0;32m    145\u001b[0m                        \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mB, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mK)\n\u001b[0;32m    146\u001b[0m scale \u001b[39m=\u001b[39m atol \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mmaximum(np\u001b[39m.\u001b[39mabs(y), np\u001b[39m.\u001b[39mabs(y_new)) \u001b[39m*\u001b[39m rtol\n\u001b[0;32m    147\u001b[0m error_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_estimate_error_norm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mK, h, scale)\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\ArticleGreenHouse\\venv\\lib\\site-packages\\scipy\\integrate\\_ivp\\rk.py:66\u001b[0m, in \u001b[0;36mrk_step\u001b[1;34m(fun, t, y, f, h, A, B, C, K)\u001b[0m\n\u001b[0;32m     63\u001b[0m     dy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(K[:s]\u001b[39m.\u001b[39mT, a[:s]) \u001b[39m*\u001b[39m h\n\u001b[0;32m     64\u001b[0m     K[s] \u001b[39m=\u001b[39m fun(t \u001b[39m+\u001b[39m c \u001b[39m*\u001b[39m h, y \u001b[39m+\u001b[39m dy)\n\u001b[1;32m---> 66\u001b[0m y_new \u001b[39m=\u001b[39m y \u001b[39m+\u001b[39m h \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mdot(K[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mT, B)\n\u001b[0;32m     67\u001b[0m f_new \u001b[39m=\u001b[39m fun(t \u001b[39m+\u001b[39m h, y_new)\n\u001b[0;32m     69\u001b[0m K[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m f_new\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "date = datetime.now().strftime(\"%Y-%m-%d__%H-%M-%S\")\n",
    "logdirs = []\n",
    "name_model = f'diff_models'\n",
    "\n",
    "MODEL_DIR = os.path.join(ENV_DIR,name_model)\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "for var in models_variants.keys():\n",
    "    name_model_var= f'var_{var}'\n",
    "\n",
    "    MODEL_DIR_VAR = os.path.join(MODEL_DIR,name_model_var)\n",
    "    if not os.path.exists(MODEL_DIR_VAR):\n",
    "        os.makedirs(MODEL_DIR_VAR)\n",
    "    \n",
    "    model_hyperparams = models_variants[var]\n",
    "    match var:\n",
    "        case 'sac':\n",
    "            model = SAC(\"MultiInputPolicy\", env, verbose=2,**model_hyperparams)\n",
    "        case 'a2c':\n",
    "            model = A2C(\"MultiInputPolicy\", env, verbose=2,**model_hyperparams)\n",
    "        case 'td3':\n",
    "            model = TD3(\"MultiInputPolicy\", env, verbose=2,**model_hyperparams)\n",
    "            \n",
    "    # Создайте директорию для записи данных TensorBoard\n",
    "    log_dir = os.path.join(\n",
    "        MODEL_DIR_VAR,\n",
    "        date\n",
    "        )  # Замените это на путь к вашей директории\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    # Создайте SummaryWriter\n",
    "    writer = create_file_writer(log_dir)\n",
    "\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps, # Общее количество шагов времени, в течение которых будет произведено обучение.\n",
    "        callback = CustomCallback(writer=writer),#  Обратный вызов (callback) для записи промежуточных результатов обучения.\n",
    "        log_interval=1e1, #Интервал для записи логов.\n",
    "        progress_bar=True\n",
    "    )\n",
    "    model.save(path=os.path.join(MODEL_DIR_VAR,f'{var}'))\n",
    "    logdirs.append(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_variants = {\n",
    "    0: {'batch_size': 64, 'ent_coef': 'auto','learning_rate': 0.003,'learning_starts': 100,'buffer_size': 100000,},\n",
    "    # 1: {'batch_size': 64, 'ent_coef': 'auto', 'learning_rate': 0.001, 'learning_starts': 100, 'buffer_size': 100000},\n",
    "    # 2: {'batch_size': 64, 'ent_coef': 0.01, 'learning_rate': 0.003, 'learning_starts': 500, 'buffer_size': 50000},\n",
    "    # 3: {'batch_size': 32, 'ent_coef': 0.005, 'learning_rate': 0.002, 'learning_starts': 200, 'buffer_size': 150000},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y-%m-%d__%H-%M-%S\")\n",
    "logdirs = []\n",
    "name_model = f'SAC'\n",
    "\n",
    "MODEL_DIR = os.path.join(ENV_DIR,name_model)\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "for var in sac_variants.keys():\n",
    "    name_model_var= f'var_{var}'\n",
    "\n",
    "    MODEL_DIR_VAR = os.path.join(MODEL_DIR,name_model_var)\n",
    "    if not os.path.exists(MODEL_DIR_VAR):\n",
    "        os.makedirs(MODEL_DIR_VAR)\n",
    "\n",
    "    sac_hyperparams = sac_variants[var]\n",
    "\n",
    "    model_sac = SAC(\"MultiInputPolicy\", env, verbose=2,**sac_hyperparams)\n",
    "    # Создайте директорию для записи данных TensorBoard\n",
    "    log_dir = os.path.join(\n",
    "        MODEL_DIR_VAR,\n",
    "        date\n",
    "        )  # Замените это на путь к вашей директории\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    # Создайте SummaryWriter\n",
    "    writer = create_file_writer(log_dir)\n",
    "\n",
    "    model_sac.learn(\n",
    "        total_timesteps=1.5e6, # Общее количество шагов времени, в течение которых будет произведено обучение.\n",
    "        callback = CustomCallback(writer=writer),#  Обратный вызов (callback) для записи промежуточных результатов обучения.\n",
    "        log_interval=1e2, #Интервал для записи логов.\n",
    "        progress_bar=True\n",
    "    )\n",
    "    model_sac.save(path=os.path.join(MODEL_DIR_VAR,f'SAC_{var}'))\n",
    "    logdirs.append(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y-%m-%d__%H-%M-%S\")\n",
    "logdirs = []\n",
    "name_model = f'SAC'\n",
    "\n",
    "MODEL_DIR = os.path.join(ENV_DIR,name_model)\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "for var in sac_variants.keys():\n",
    "    name_model_var= f'var_{var}'\n",
    "\n",
    "    MODEL_DIR_VAR = os.path.join(MODEL_DIR,name_model_var)\n",
    "    if not os.path.exists(MODEL_DIR_VAR):\n",
    "        os.makedirs(MODEL_DIR_VAR)\n",
    "\n",
    "    sac_hyperparams = sac_variants[var]\n",
    "\n",
    "    model_sac = SAC(\"MultiInputPolicy\", env, verbose=2,**sac_hyperparams)\n",
    "    # Создайте директорию для записи данных TensorBoard\n",
    "    log_dir = os.path.join(\n",
    "        MODEL_DIR_VAR,\n",
    "        date\n",
    "        )  # Замените это на путь к вашей директории\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    # Создайте SummaryWriter\n",
    "    writer = create_file_writer(log_dir)\n",
    "\n",
    "    model_sac.learn(\n",
    "        total_timesteps=1.5e6, # Общее количество шагов времени, в течение которых будет произведено обучение.\n",
    "        callback = CustomCallback(writer=writer),#  Обратный вызов (callback) для записи промежуточных результатов обучения.\n",
    "        log_interval=1e2, #Интервал для записи логов.\n",
    "        progress_bar=True\n",
    "    )\n",
    "    model_sac.save(path=os.path.join(MODEL_DIR_VAR,f'SAC_{var}'))\n",
    "    logdirs.append(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard --logdir={MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_variants = {\n",
    "    0: {'n_steps': 5,'ent_coef': 0.01,'learning_rate': 0.0007,'vf_coef': 0.25,'max_grad_norm': 0.5,},\n",
    "    1: {'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.001, 'vf_coef': 0.2, 'max_grad_norm': 0.5},\n",
    "    2: {'n_steps': 10, 'ent_coef': 0.015, 'learning_rate': 0.0005, 'vf_coef': 0.3, 'max_grad_norm': 0.6},\n",
    "    3: {'n_steps': 5, 'ent_coef': 0.02, 'learning_rate': 0.0007, 'vf_coef': 0.15, 'max_grad_norm': 0.7},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y-%m-%d__%H-%M-%S\")\n",
    "\n",
    "name_model = f'A2C'\n",
    "\n",
    "MODEL_DIR = os.path.join(ENV_DIR,name_model)\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "for var in a2c_variants.keys():\n",
    "    name_model_var= f'var_{var}'\n",
    "\n",
    "    MODEL_DIR_VAR = os.path.join(MODEL_DIR,name_model_var)\n",
    "    if not os.path.exists(MODEL_DIR_VAR):\n",
    "        os.makedirs(MODEL_DIR_VAR)\n",
    "\n",
    "    a2c_hyperparams = a2c_variants[var]\n",
    "    model_a2c = A2C(\"MultiInputPolicy\", env, verbose=1,**a2c_hyperparams)\n",
    "\n",
    "    # Создайте директорию для записи данных TensorBoard\n",
    "    log_dir = os.path.join(\n",
    "        MODEL_DIR_VAR,\n",
    "        date\n",
    "        )  # Замените это на путь к вашей директории\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    # Создайте SummaryWriter\n",
    "    writer = create_file_writer(log_dir)\n",
    "\n",
    "    model_a2c.learn(\n",
    "        total_timesteps=1e0, # Общее количество шагов времени, в течение которых будет произведено обучение.\n",
    "        callback = CustomCallback(writer=writer),#  Обратный вызов (callback) для записи промежуточных результатов обучения.\n",
    "        log_interval=1e2, #Интервал для записи логов.\n",
    "        progress_bar=True\n",
    "    )\n",
    "    model_a2c.save(path=os.path.join(MODEL_DIR_VAR,f'PPO_{var}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_variants = {\n",
    "    0: {\n",
    "    'learning_rate' : 0.001,\n",
    "    'n_steps' :2048,\n",
    "    'ent_coef' : 0.01,\n",
    "    'vf_coef' : 0.5\n",
    "},\n",
    "    1: {\n",
    "    'learning_rate': 0.0005,\n",
    "    'n_steps' : 1024,\n",
    "    'ent_coef':  0.05,\n",
    "    'vf_coef' : 0.5\n",
    "},\n",
    "    2: {\n",
    "    'learning_rate'  : 0.002,\n",
    "    'n_steps'  :  4096,\n",
    "    'ent_coef'  : 0.005,\n",
    "    'vf_coef' : 0.5\n",
    "}\n",
    ",\n",
    "    3: {\n",
    "        'learning_rate' : 0.001,\n",
    "    'n_steps' : 2048,\n",
    "    'ent_coef' : 0.1,\n",
    "    'vf_coef' : 0.5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y-%m-%d__%H-%M-%S\")\n",
    "\n",
    "name_model = f'PPO'\n",
    "\n",
    "MODEL_DIR = os.path.join(ENV_DIR,name_model)\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "    \n",
    "for var in ppo_variants.keys():\n",
    "    name_model_var = f'var_{var}'\n",
    "\n",
    "    MODEL_DIR_VAR = os.path.join(MODEL_DIR,name_model_var)\n",
    "    if not os.path.exists(MODEL_DIR_VAR):\n",
    "        os.makedirs(MODEL_DIR_VAR)\n",
    "\n",
    "    ppo_hyperparams = ppo_variants[var]\n",
    "    model_ppo = PPO(\"MultiInputPolicy\", env, verbose=1,**ppo_hyperparams)\n",
    "\n",
    "    # Создайте директорию для записи данных TensorBoard\n",
    "    log_dir = os.path.join(\n",
    "        MODEL_DIR_VAR,\n",
    "        date\n",
    "        )  # Замените это на путь к вашей директории\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    # Создайте SummaryWriter\n",
    "    writer = create_file_writer(log_dir)\n",
    "\n",
    "    model_ppo.learn(\n",
    "        total_timesteps=1,#e6, # Общее количество шагов времени, в течение которых будет произведено обучение.\n",
    "        callback = CustomCallback(writer=writer),#  Обратный вызов (callback) для записи промежуточных результатов обучения.\n",
    "        log_interval=1e2, #Интервал для записи логов.\n",
    "        progress_bar=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_variants = {\n",
    "    0: {\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"buffer_size\": 500_000,\n",
    "    \"learning_starts\": 200,\n",
    "    \"batch_size\": 64,\n",
    "    \"tau\": 0.01,\n",
    "    \"gamma\": 0.98,\n",
    "    \"train_freq\": (2, \"episode\"),\n",
    "    \"gradient_steps\": 50,\n",
    "    \"policy_delay\": 3,\n",
    "    \"target_policy_noise\": 0.15,\n",
    "    \"target_noise_clip\": 0.4,\n",
    "    \"stats_window_size\": 50,\n",
    "}\n",
    ",\n",
    "    1: {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"buffer_size\": 1_000_000,\n",
    "    \"learning_starts\": 100,\n",
    "    \"batch_size\": 100,\n",
    "    \"tau\": 0.005,\n",
    "    \"gamma\": 0.99,\n",
    "    \"train_freq\": (4, \"step\"),\n",
    "    \"gradient_steps\": 100,\n",
    "    \"policy_delay\": 1,\n",
    "    \"target_policy_noise\": 0.1,\n",
    "    \"target_noise_clip\": 0.3,\n",
    "    \"stats_window_size\": 200,\n",
    "}\n",
    ",\n",
    "    2: {\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"buffer_size\": 750_000,\n",
    "    \"learning_starts\": 150,\n",
    "    \"batch_size\": 75,\n",
    "    \"tau\": 0.0075,\n",
    "    \"gamma\": 0.97,\n",
    "    \"train_freq\": (3, \"episode\"),\n",
    "    \"gradient_steps\": 75,\n",
    "    \"policy_delay\": 2,\n",
    "    \"target_policy_noise\": 0.125,\n",
    "    \"target_noise_clip\": 0.35,\n",
    "    \"stats_window_size\": 150,\n",
    "}\n",
    ",\n",
    "    3: {\n",
    "    \"learning_rate\": 3e-3,\n",
    "    \"buffer_size\": 1_250_000,\n",
    "    \"learning_starts\": 50,\n",
    "    \"batch_size\": 50,\n",
    "    \"tau\": 0.0025,\n",
    "    \"gamma\": 0.95,\n",
    "    \"train_freq\": (1, \"step\"),\n",
    "    \"gradient_steps\": 200,\n",
    "    \"policy_delay\": 4,\n",
    "    \"target_policy_noise\": 0.2,\n",
    "    \"target_noise_clip\": 0.45,\n",
    "    \"stats_window_size\": 250,\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y-%m-%d__%H-%M-%S\")\n",
    "\n",
    "name_model = f'TD3'\n",
    "\n",
    "MODEL_DIR = os.path.join(ENV_DIR,name_model)\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "    \n",
    "for var in td3_variants.keys():\n",
    "    name_model_var = f'var_{var}'\n",
    "\n",
    "    MODEL_DIR_VAR = os.path.join(MODEL_DIR,name_model_var)\n",
    "    if not os.path.exists(MODEL_DIR_VAR):\n",
    "        os.makedirs(MODEL_DIR_VAR)\n",
    "        \n",
    "    td3_hyperparams = td3_variants[var]\n",
    "    model_td3 = TD3(\"MultiInputPolicy\", env, verbose=1,**td3_hyperparams)\n",
    "\n",
    "    # Создайте директорию для записи данных TensorBoard\n",
    "    log_dir = os.path.join(\n",
    "        MODEL_DIR_VAR,\n",
    "        date\n",
    "        )  # Замените это на путь к вашей директории\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    # Создайте SummaryWriter\n",
    "    writer = create_file_writer(log_dir)\n",
    "\n",
    "    model_td3.learn(\n",
    "        total_timesteps=1e0, # Общее количество шагов времени, в течение которых будет произведено обучение.\n",
    "        callback = CustomCallback(writer=writer),#  Обратный вызов (callback) для записи промежуточных результатов обучения.\n",
    "        log_interval=1e2, #Интервал для записи логов.\n",
    "        progress_bar=True\n",
    "    )\n",
    "    model_td3.save(path=os.path.join(MODEL_DIR_VAR,f'TD3_{var}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = env.optimal_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._get_observation( env._model.get_output())['crop_model']['PAVAIL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "def get_values(env):\n",
    "    T_soil_arr = env.T_soil\n",
    "    humid_soil_arr = env.humid_soil\n",
    "    pH_arr = env.pH\n",
    "    EC_arr = env.EC\n",
    "    dV_arr = env.dV\n",
    "    NAVAIL_arr = env._get_observation( env._model.get_output())['crop_model']['NAVAIL']\n",
    "    PAVAIL_arr = env._get_observation( env._model.get_output())['crop_model']['PAVAIL']\n",
    "    KAVAIL_arr = env._get_observation( env._model.get_output())['crop_model']['KAVAIL']\n",
    "    return {\n",
    "        'T_soil': T_soil_arr,\n",
    "        'humid_soil': humid_soil_arr,\n",
    "        'pH': pH_arr,\n",
    "        'EC': EC_arr,\n",
    "        'NAVAIL': np.array(NAVAIL_arr),\n",
    "        'PAVAIL':  np.array(PAVAIL_arr),\n",
    "        'KAVAIL': np.array(KAVAIL_arr),\n",
    "        'dV':  np.array([ dV_arr])\n",
    "    }\n",
    "o_init = get_values(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 100\n",
    "for _ in range(timesteps):\n",
    "    a = env.action_space.sample()\n",
    "    o, r, terminated, truncated, info = env.step(a)\n",
    "    values = {\n",
    "        'T_soil': env.T_soil,\n",
    "        'humid_soil': env.humid_soil,\n",
    "        'pH': env.pH,\n",
    "        'EC': env.EC,\n",
    "        'NAVAIL': np.array(o['crop_model']['NAVAIL']),\n",
    "        'PAVAIL': np.array(o['crop_model']['PAVAIL']),\n",
    "        'KAVAIL': np.array(o['crop_model']['KAVAIL']),\n",
    "        'dV': np.array([env.dV])\n",
    "    }\n",
    "    for key in values.keys():\n",
    "        if isinstance(o_init[key], np.ndarray):\n",
    "            o_init[key] = np.append(o_init[key], values[key])\n",
    "        # else:\n",
    "        #     o_init[key] = o_init[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in o_init.keys():\n",
    "    print(o_init[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_init_sh =o_init\n",
    "for key in o_init.keys():\n",
    "    o_init_sh[key] = o_init[key].reshape(1,-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем временной ряд для оси X (в часах)\n",
    "time_hours = np.linspace(0,timesteps,101)\n",
    "\n",
    "# Создаем графики\n",
    "fig, axs = plt.subplots(len(a), 1, figsize=(8, 6 * len(a)), sharex=True)\n",
    "\n",
    "for i, (key, values) in enumerate(o_init.items()):\n",
    "    if key  != 'dV':\n",
    "        ax = axs[i]\n",
    "        ax.set_title(key)\n",
    "\n",
    "        # Горизонтальные линии из словаря a\n",
    "        for value in a[key]:\n",
    "            ax.axhline(y=value, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "        # Графики значений из словаря b\n",
    "        ax.plot(time_hours, o_init[key])\n",
    "\n",
    "        # Настройка меток осей\n",
    "        ax.set_xlabel('Время')\n",
    "        ax.set_ylabel(key)\n",
    "        ax.grid()\n",
    "\n",
    "# Устанавливаем общий заголовок для всех подграфиков\n",
    "fig.suptitle('Горизонтальные линии и ряды значений')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC.load(path=os.path.join(os.getcwd(),'tensorboard_logs','GreenHousePCSE','SAC','var_0','SAC_0'),env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy \n",
    "\n",
    "a = model.predict(env.reset()[0])[0]\n",
    "env.step(a)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 100\n",
    "o,_ = env.reset()\n",
    "# o = env.reshape_obs_dict(o)\n",
    "obs = o\n",
    "\n",
    "for i in range(timesteps):\n",
    "    print(f'{i} {obs}')\n",
    "    a,_ = model.predict(obs)\n",
    "    obs, r, terminated, truncated, info = env.step(a)\n",
    "    # for key in env._additional_variables:\n",
    "    #     o['crop_model'][key] = o[key]\n",
    "    #     del o[key]\n",
    "    obs= env.reshape_obs_dict(obs)\n",
    "    print(f'{i} {obs}')\n",
    "    # values = {\n",
    "    #     'T_soil': env.T_soil,\n",
    "    #     'humid_soil': env.humid_soil,\n",
    "    #     'pH': env.pH,\n",
    "    #     'EC': env.EC,\n",
    "    #     'NAVAIL': np.array(o['crop_model']['NAVAIL']),\n",
    "    #     'PAVAIL': np.array(o['crop_model']['PAVAIL']),\n",
    "    #     'KAVAIL': np.array(o['crop_model']['KAVAIL']),\n",
    "    #     'dV': np.array([env.dV])\n",
    "    # obs = env.reshape_obs_dict(obs)\n",
    "    # }\n",
    "    # for key in values.keys():\n",
    "    #     if isinstance(o_init[key], np.ndarray):\n",
    "    #         o_init[key] = np.append(o_init[key], values[key])\n",
    "        # else:\n",
    "        #     o_init[key] = o_init[key]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
